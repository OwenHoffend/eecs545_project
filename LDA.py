# -*- coding: utf-8 -*-
"""LDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NqaR37beAJisogi9k0ro2GCnu1_Ihjf8

# Apply LDA to OSHA
"""

#Install packages 
#!pip install pyldavis

data_path = "your_own_path"
data_file_lst = os.listdir(data_path)

print("data_file_lst: {}".format(data_file_lst))

import pandas as pd
df = pd.read_excel(data_path+'tagged1000.xlsx')
print(len(df))

df.head()

df.dropna(subset=['summary.new', 'Tagged2'])

print(len(df))

df.head(5)

summary = df['summary.new']
summary.head()

print("Number of tags: {}".format(len(df.Tagged2.unique())))
frequency = df.Tagged2.value_counts()
frequency

y_train_df = df['Tagged2']

"""## Preprocessing: Remove punctuation, lower alphabet"""

import re
summary = summary.apply(lambda x: x.strip())
# Remove punctuation
summary = summary.apply(lambda x: re.sub('[.,\!]', '', str(x)))
# Lower the letter
summary = summary.apply(lambda x: x.lower())
summary.head()

"""## Word Cloud - Visualization"""

summary_combined_string = ','.join(list(summary.values))
summary_combined_string

# Combine 'summary' sentence into one string
from wordcloud import WordCloud
summary_combined_string = ','.join(list(summary.values))
# Create a word cloud object
summary_wordcloud = WordCloud(width=500,height=300,max_words=5000, contour_width=2, background_color="white", collocations=False)
# Word cloud generation
summary_wordcloud.generate(summary_combined_string)
# Visualize the word cloud
summary_wordcloud.to_image()

"""## LDA"""

import nltk

nltk.download('stopwords')
from nltk.corpus import stopwords

import gensim
from gensim.utils import simple_preprocess

stop_words = stopwords.words('english')
extend_lst = ['from', '']

stop_words.extend(extend_lst)
print(stop_words)

import gensim.corpora as corpora

from pprint import pprint

summary_tolist = summary.tolist()
#summary_tolist

summary_to_wordlist = []

## Remove Stopwords & Blank
for i in range(len(summary_tolist)):
  sent = summary_tolist[i]
  empty_lst = []
  for word in sent.split(" "): 
    if (word != stop_words) and (word !="") :
      empty_lst.append(word)
  
  summary_to_wordlist.append(empty_lst)

#summary_to_wordlist

print(summary_to_wordlist[:1])

# Create Dictionary
summary_id2word = corpora.Dictionary(summary_to_wordlist)
# Create Corpus
summary_texts = summary_to_wordlist
# Term Document Frequency
summary_corpus = [summary_id2word.doc2bow(text) for text in summary_texts]
# View
print(summary_corpus[:1][0])

texts = [['human', 'interface', 'computer'],
         ['survey', 'user', 'computer', 'system', 'response', 'time'],
         ['eps', 'user', 'interface', 'system'],
         ['system', 'human', 'system', 'eps'],
         ['user', 'response', 'time'],
         ['trees'],
         ['graph', 'trees','trees','trees'],
         ['graph', 'minors', 'trees'],
         ['graph', 'minors', 'survey']]

dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]
corpus

print(summary_corpus[:5])

from pprint import pprint
# number of topics
topic_num = 11
# Build LDA model
summary_lda_model = gensim.models.LdaMulticore(corpus=summary_corpus,
                                       id2word=summary_id2word,
                                       num_topics=topic_num)

# Print the Keyword in the 10 topics
pprint(summary_lda_model.print_topics())
summary_doc_lda = summary_lda_model[summary_corpus]

"""## Visualize LDA results - with pyLDAvis"""

#!pip install pyLDAvis

#!pip install --upgrade pandas==1.2

"""## Model Perplexity and Coherence Score"""

#Condition to find optimized topics 
start = 2
end = 20
interval = 1

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

"""##### Coherence Graph"""

from gensim.models import CoherenceModel
summary_conherence_lst = []
summary_perplexity_lst = [] 
summary_model_lst = []

for topic_num in range(start,end,interval):
  temp_summary_lda_model = gensim.models.LdaMulticore(corpus=summary_corpus,
                                       id2word=summary_id2word,
                                       num_topics=topic_num)

  summary_model_lst.append(temp_summary_lda_model)

  # coherence
  coherence_model_lda = CoherenceModel(model=temp_summary_lda_model, texts=summary_to_wordlist, dictionary=summary_id2word, coherence='c_v')
  
  ### coherence score
  coherence_lda_value = coherence_model_lda.get_coherence()
  summary_conherence_lst.append(coherence_lda_value)

  # perplexity
  ### coherence score
  perplexity_value = temp_summary_lda_model.log_perplexity(summary_corpus)
  summary_perplexity_lst.append(perplexity_value)

"""##### Coherence Graph"""

## coherence graph
x_value = range(start,end,interval)

plt.plot(x_value,summary_conherence_lst)
plt.xlabel("Number of Topics")
plt.ylabel("Coherence score")

plt.xlim([2,20])
plt.show()

for m, cv in zip(x_value,summary_conherence_lst):
  print("Number of Topics = ",m, " has coherence value of", round(cv, 4))

"""##Analyze with optimal model"""

summary_optimal_model = summary_model_lst[9]
summary_topic_in_model = summary_optimal_model.show_topics(formatted=False)
#pprint(summary_optimal_model.print_topics(num_words=10))

"""##### Representative title sentence for each topic group"""

value = 20
tag2idx = {t : i+value for i,t in enumerate(list(set(y_train_df.values)))}
tag2idx

import numpy as np
y_train = y_train_df.to_numpy()
y_train_number = np.zeros(y_train.shape)
for i in range(y_train_number.shape[0]):
  y_train_number[i,] = tag2idx[y_train[i,]]
y_train_number = y_train_number.astype(int)
#y_train_number

# Init output
df_represent_summary_sent = pd.DataFrame()

summary_optimal_topic_num = 10

# Get main topic in each document
for i, perc in enumerate(summary_optimal_model[summary_corpus]):
    perc = sorted(perc, key=lambda x: (x[1]), reverse=True)
    # Get the Dominant topic, Contribution and Keywords for each document
    for j, (summary_optimal_topic_num, topic_prec) in enumerate(perc):
        if j == 0:  # => dominant topic
            wp = summary_optimal_model.show_topic(summary_optimal_topic_num)
            topic_keywords = ", ".join([word for word, prop in wp])
            df_represent_summary_sent = df_represent_summary_sent.append(pd.Series([int(summary_optimal_topic_num), round(topic_prec,4), topic_keywords]), ignore_index=True)

df_represent_summary_sent.columns = ['Dominant_Topic', 'Contribution', 'Keywords']

# Add original text to the end of the output
contents = pd.Series(summary)
gt_label = pd.Series(y_train_df)
gt_label_number = pd.Series(y_train_number)
df_represent_summary_sent = pd.concat([df_represent_summary_sent, contents,gt_label,gt_label_number], axis=1)
df_represent_summary_sent

df_summary_dominant_topic = df_represent_summary_sent.reset_index()
df_summary_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Summary','GT_label','GT_label_number']

#df_summary_dominant_topic.head(10)

df_summary_dominant_topic['Summary'][1]

# Group top 5 sentences under each topic
df_summary_group_top_5 = pd.DataFrame()

summary_sent_topics_outdf_grpd = df_represent_summary_sent.groupby('Dominant_Topic')

for i, grp in summary_sent_topics_outdf_grpd:
    df_summary_group_top_5 = pd.concat([df_summary_group_top_5, 
                                             grp.sort_values(['Contribution'], ascending=[0]).head(1)], 
                                            axis=0)

# Reset Index    
df_summary_group_top_5.reset_index(drop=True, inplace=True)

# Format
df_summary_group_top_5.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", 'Summary','GT_label','GT_label_number']

# Show
#df_summary_group_top_5



# https://medium.com/@joel_34096/k-means-clustering-for-image-classification-a648f28bdc47
def retrieve_info(cluster_labels,y_train):
  """Associates most probable label with each cluster in KMeans model
  returns: dictionary of clusters assigned to each label
  """
  # Initializing
  reference_labels = {}
  # For loop to run through each label of cluster label
  for i in range(len(np.unique(pred_np))):
    index = np.where(cluster_labels == i,1,0)
    num = np.bincount(y_train[index==1]).argmax()
    reference_labels[i] = num
  return reference_labels

pred_np = df_summary_dominant_topic['Dominant_Topic'].to_numpy()
gt_np_name = df_summary_dominant_topic['GT_label'].to_numpy()
gt_np = df_summary_dominant_topic['GT_label_number'].to_numpy()

pred_np = pred_np.astype('int')
#pred_np

#gt_np

# https://medium.com/@joel_34096/k-means-clustering-for-image-classification-a648f28bdc47
##def retrieve_info(cluster_labels,y_train):
##  """Associates most probable label with each cluster in KMeans model
##  returns: dictionary of clusters assigned to each label
##  """
##  # Initializing
##  reference_labels = {}
  # For loop to run through each label of cluster label
##  for i in range(len(np.unique(cluster_labels))):
##    index = np.where(cluster_labels == i,1,0)
##    try:
##      num = np.bincount(y_train[index==1]).argmax()
##      reference_labels[i] = num
##    except ValueError:
##      pass
##  return reference_labels

#len(np.unique(pred_np))

reference_labels = {}
# For loop to run through each label of cluster label
#for i in range(len(np.unique(pred_np))):
for i in range(len(np.unique(gt_np))):
  index = np.where(pred_np == i,1,0)
  #print(index)
  try:
    num = np.bincount(gt_np[index==1]).argmax()
    reference_labels[i] = num
  except ValueError:
    pass
#reference_labels

reference_labels[3]=0
#reference_labels

import numpy as np
#reference_labels = retrieve_info(pred_np,gt_np)
number_labels = np.random.rand(len(pred_np))

for i in range(len(pred_np)):
  number_labels[i] = reference_labels[pred_np[i]]

print(number_labels[:20].astype('int'))
print(gt_np[:20])
print(gt_np_name[:20])

## 다시 한 번 출처: https://medium.com/@joel_34096/k-means-clustering-for-image-classification-a648f28bdc47
from sklearn.metrics import accuracy_score
print(accuracy_score(number_labels,gt_np))

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#groud truth: y_train_number

# print accuracy
print("Accuracy: ", accuracy_score(gt_np, number_labels))

# print precision, recall, F1-score per each class/tag
print(classification_report(gt_np, number_labels))

# print confusion matrix, check documentation for sorting rows/columns
print(confusion_matrix(gt_np, number_labels))

print(tag2idx)



